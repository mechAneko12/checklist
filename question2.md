# 問２
## (1) アルゴリズム
> - **最尤推定法**
> 教師あり学習。ある確率分布$q(x_i;\theta)$から生起する確率密度、すなわちすべてのカテゴリの確率分布の積を以下のように尤度$L(\theta)$とする。
> $$ L(\theta)=\prod_{i = 1}^{n}q(x_i;\theta)$$この値が最大のときに最も尤もらしい分布といえる。尤度が最大の時に偏微分の値が0になるという性質から、その時のパラメータ$\theta$を求めるための方程式は以下のようになる。これを尤度方程式という。
> $$ \frac{\partial}{\partial θ}\textrm{log}L(\theta)=0$$尤度方程式を解いて尤度を最大にするようにパラメータ$\theta$を決定する。決定したパラメータに基づいて事後確率を求め、事後確率が最大のカテゴリに分類する。
> 
> - **$k$近傍法**
> 教師あり学習。最も簡単な仕組みの機械学習の手法である。パラメータによって定義されたベクトル空間の点から（一般的に）ユークリッド距離で最も近い$k$個の点がどのクラスに割り当てられているかを確認し、もっとの多いクラスに分類する。
> 
> - $k$近傍法
> 教師なし学習。以下の手順でクラスタに分類する。
>	1. 各点$x_i$をランダムに$k$個のクラスタに分類する。
>	2. 各クラスタの重心を求める。
>	3. 各点$x_i$と各クラスタの重心の距離を求め、距離の近いクラスタに振り分ける。
>	4. 振り分けられるクラスタが変化しなくなる、または変動が閾値を下回った時に終了する。

## (2.2) **numpy 2d array X_new**

> ```python:question2.py
> X_new = np.concatenate((np.ones((X.shape[0],1)), X), axis=1)
> ```

## (3.2) **ロジステック回帰 f(x, b)**